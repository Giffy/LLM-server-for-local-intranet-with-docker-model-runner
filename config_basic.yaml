name: Local Config
version: 1.0.0
schema: v1
models:
  # Configuration for low spec server with 8Gb and without GPU
  
  - name: dockerserver-granite4
    provider: llamastack
    model: ai/granite-4.0-nano:350M-BF16
    apiBase: http://192.168.10.36:12435/engines/llama.cpp/v1/
    roles:
      - chat
    defaultCompletitionOptions:
      temperature: 0.1
      maxTokens: 300

  - name: dockerserver-gemma3
    provider: llamastack
    model: ai/gemma3:270M-F16
    apiBase: http://192.168.10.36:12435/engines/llama.cpp/v1/
    roles:
      - chat
    defaultCompletitionOptions:
      temperature: 0.1
      maxTokens: 300


  - name: dockerserver-qwen-without thinking
    provider: llamastack
    model: qwen2.5:0.5B-F16
    apiBase: http://192.168.10.36:12435/engines/llama.cpp/v1/
    roles:
      - autocomplete
    defaultCompletitionOptions:
      temperature: 0.1
      maxTokens: 300      
    requestOptions:
      extraBodyProperties:
        think: false # turning off the thinking

rules:
  - Give concise responses
  - Always describe the decissions taken

prompts:
  - name: Software developer
    prompt: I am a senior software developer.